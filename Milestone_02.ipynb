{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Milestone Two: Modeling and Feature Engineering\n",
    "\n",
    "### Due: Midnight on August 3 (with 2-hour grace period) and worth 50 points\n",
    "\n",
    "### Overview\n",
    "\n",
    "This milestone builds on your work from Milestone 1 and will complete the coding portion of your project. You will:\n",
    "\n",
    "1. Pick 3 modeling algorithms from those we have studied.\n",
    "2. Evaluate baseline models using default settings.\n",
    "3. Engineer new features and re-evaluate models.\n",
    "4. Use feature selection techniques and re-evaluate.\n",
    "5. Fine-tune for optimal performance.\n",
    "6. Select your best model and report on your results. \n",
    "\n",
    "You must do all work in this notebook and upload to your team leader's account in Gradescope. There is no\n",
    "Individual Assessment for this Milestone. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# Useful Imports: Add more as needed\n",
    "# ===================================\n",
    "\n",
    "# Standard Libraries\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "from itertools import chain, combinations\n",
    "\n",
    "# Data Science Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.ticker as mticker  # Optional: Format y-axis labels as dollars\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn (Machine Learning)\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, \n",
    "    cross_val_score, \n",
    "    GridSearchCV, \n",
    "    RandomizedSearchCV, \n",
    "    RepeatedKFold\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_selection import SequentialFeatureSelector, f_regression, SelectKBest\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "# Progress Tracking\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =============================\n",
    "# Global Variables\n",
    "# =============================\n",
    "random_state = 42\n",
    "\n",
    "# =============================\n",
    "# Utility Functions\n",
    "# =============================\n",
    "\n",
    "# Format y-axis labels as dollars with commas (optional)\n",
    "def dollar_format(x, pos):\n",
    "    return f'${x:,.0f}'\n",
    "\n",
    "# Convert seconds to HH:MM:SS format\n",
    "def format_hms(seconds):\n",
    "    return time.strftime(\"%H:%M:%S\", time.gmtime(seconds))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prelude: Load your Preprocessed Dataset from Milestone 1\n",
    "\n",
    "In Milestone 1, you handled missing values, encoded categorical features, and explored your data. Before you begin this milestone, you’ll need to load that cleaned dataset and prepare it for modeling. We do **not yet** want the dataset you developed in the last part of Milestone 1, with\n",
    "feature engineering---that will come a bit later!\n",
    "\n",
    "Here’s what to do:\n",
    "\n",
    "1. Return to your Milestone 1 notebook and rerun your code through Part 3, where your dataset was fully cleaned (assume it’s called `df_cleaned`).\n",
    "\n",
    "2. **Save** the cleaned dataset to a file by running:\n",
    "\n",
    ">   df_cleaned.to_csv(\"zillow_cleaned.csv\", index=False)\n",
    "\n",
    "3. Switch to this notebook and **load** the saved data:\n",
    "\n",
    ">   df = pd.read_csv(\"zillow_cleaned.csv\")\n",
    "\n",
    "4. Create a **train/test split** using `train_test_split`.  \n",
    "   \n",
    "6. **Standardize** the features (but not the target!) using **only the training data.** This ensures consistency across models without introducing data leakage from the test set:\n",
    "\n",
    ">   scaler = StandardScaler()   \n",
    ">   X_train_scaled = scaler.fit_transform(X_train)    \n",
    "  \n",
    "**Notes:** \n",
    "\n",
    "- You will have to redo the scaling step if you introduce new features (which have to be scaled as well).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add as many cells as you need\n",
    "\n",
    "# Step 1-3\n",
    "df = pd.read_csv(\"zillow_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62060, 22) (15516, 22) (62060,) (15516,)\n"
     ]
    }
   ],
   "source": [
    "# Step 4\n",
    "X = df.drop(columns=[\"taxvaluedollarcnt\"])   # all features\n",
    "y = df[\"taxvaluedollarcnt\"]                  # target\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=random_state\n",
    ")\n",
    "\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62060, 22) (15516, 22)\n"
     ]
    }
   ],
   "source": [
    "# 5a) Numeric vs. categorical columns\n",
    "numeric_cols     = X_train.select_dtypes(include=[np.number]).columns\n",
    "categorical_cols = X_train.select_dtypes(exclude=[np.number]).columns\n",
    "\n",
    "# 5b) Median for numerics, most frequent for categoricals\n",
    "num_imp = SimpleImputer(strategy=\"median\")\n",
    "cat_imp = SimpleImputer(strategy=\"most_frequent\")\n",
    "\n",
    "X_train_num = pd.DataFrame(\n",
    "    num_imp.fit_transform(X_train[numeric_cols]),\n",
    "    columns=numeric_cols,\n",
    "    index=X_train.index\n",
    ")\n",
    "X_test_num  = pd.DataFrame(\n",
    "    num_imp.transform(X_test[numeric_cols]),\n",
    "    columns=numeric_cols,\n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "X_train_cat = pd.DataFrame(\n",
    "    cat_imp.fit_transform(X_train[categorical_cols]),\n",
    "    columns=categorical_cols,\n",
    "    index=X_train.index\n",
    ")\n",
    "X_test_cat  = pd.DataFrame(\n",
    "    cat_imp.transform(X_test[categorical_cols]),\n",
    "    columns=categorical_cols,\n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "# 5c) Scale the numeric block\n",
    "scaler = StandardScaler()\n",
    "X_train_num_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(X_train_num),\n",
    "    columns=numeric_cols,\n",
    "    index=X_train.index\n",
    ")\n",
    "X_test_num_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_test_num),\n",
    "    columns=numeric_cols,\n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "# 5d) Ordinal‐encode to (now imputed) categoricals,\n",
    "#   \n",
    "encoder = OrdinalEncoder(\n",
    "    handle_unknown=\"use_encoded_value\",\n",
    "    unknown_value=-1\n",
    ")\n",
    "X_train_cat_enc = pd.DataFrame(\n",
    "    encoder.fit_transform(X_train_cat),\n",
    "    columns=categorical_cols,\n",
    "    index=X_train.index\n",
    ")\n",
    "X_test_cat_enc  = pd.DataFrame(\n",
    "    encoder.transform(X_test_cat),\n",
    "    columns=categorical_cols,\n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "# 5e) Final\n",
    "X_train_scaled = pd.concat([X_train_num_scaled, X_train_cat_enc], axis=1)\n",
    "X_test_scaled  = pd.concat([X_test_num_scaled, X_test_cat_enc],   axis=1)\n",
    "\n",
    "print(X_train_scaled.shape, X_test_scaled.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Picking Three Models and Establishing Baselines [6 pts]\n",
    "\n",
    "Apply the following regression models to the scaled training dataset using **default parameters** for **three** of the models we have worked with this term:\n",
    "\n",
    "- Linear Regression\n",
    "- Ridge Regression\n",
    "- Lasso Regression\n",
    "- Decision Tree Regression\n",
    "- Bagging\n",
    "- Random Forest\n",
    "- Gradient Boosting Trees\n",
    "\n",
    "For each of the three models:\n",
    "- Use **repeated cross-validation** (e.g., 5 folds, 5 repeats).\n",
    "- Report the **mean and standard deviation of CV MAE Score**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression     → MAE: 243773 ± 3343\n",
      "Bagging Regressor    → MAE: 198207 ± 2761\n",
      "Gradient Boosting    → MAE: 200104 ± 2555\n"
     ]
    }
   ],
   "source": [
    "# 3 Models are ridge, bagging, gbr\n",
    "ridge = Ridge(random_state=random_state)\n",
    "bagging = BaggingRegressor(random_state=random_state)\n",
    "gbr = GradientBoostingRegressor(random_state=random_state)\n",
    "\n",
    "models = [\n",
    "    (\"Ridge Regression\", ridge),\n",
    "    (\"Bagging Regressor\", bagging),\n",
    "    (\"Gradient Boosting\", gbr),\n",
    "]\n",
    "\n",
    "# Repeated CV (5 folds x 5 repeats)\n",
    "rkf = RepeatedKFold(\n",
    "    n_splits=5,\n",
    "    n_repeats=5,\n",
    "    random_state=random_state\n",
    ")\n",
    "\n",
    "# Evaluate each model\n",
    "for name, model in models:\n",
    "    neg_mae_scores = cross_val_score(\n",
    "        model,\n",
    "        X_train_scaled,\n",
    "        y_train,\n",
    "        scoring=\"neg_mean_absolute_error\",\n",
    "        cv=rkf,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    mae_scores = -neg_mae_scores  \n",
    "    print(f\"{name:20s} → MAE: {mae_scores.mean():.0f} ± {mae_scores.std():.0f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest → MAE: 190862 ± 2583\n"
     ]
    }
   ],
   "source": [
    "# Added Random Forest to test (all had high MAE)\n",
    "rf = RandomForestRegressor(random_state=random_state)\n",
    "\n",
    "neg_mae = cross_val_score(\n",
    "    rf,\n",
    "    X_train_scaled,     \n",
    "    y_train,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    cv=rkf,              \n",
    "    n_jobs=-1\n",
    ")\n",
    "mae = -neg_mae\n",
    "print(f\"Random Forest → MAE: {mae.mean():.0f} ± {mae.std():.0f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Bar Chart to compare / evaluate \n",
    "\n",
    "models = [\"Ridge\", \"Bagging\", \"GBM\", \"RF\"]\n",
    "means  = [244000, 198000, 200000, 191000]  # your exact means\n",
    "stds   = [3300,   2760,   2555,   2583]    # your exact stds\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar(models, means, yerr=stds, capsize=5)\n",
    "plt.ylabel(\"Mean MAE\")\n",
    "plt.title(\"Model Comparison: Mean ±1o MAE\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Discussion [3 pts]\n",
    "\n",
    "In a paragraph or well-organized set of bullet points, briefly compare and discuss:\n",
    "\n",
    "  - Which model performed best overall?\n",
    "  - Which was most stable (lowest std)?\n",
    "  - Any signs of overfitting or underfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random Forest Model was the best overall performer in terms of mean absolute error with roughly 191,000 and it had the smallest standard deviation with 2,600. The worst performer was the Ridge model with approximately 244,000 in terms of MAE and was the least stable at 3,300 standard deviation. This makes sense because of the numerous complex variables used in predicting house prices that it would not be simple enought to fit a linear model like the ridge for this situation. I decided to try a fourth model with the random forest because the MAE were all very large for the first three models selected and I wanted to explore more. While the random forest helped improve the MAE there were no large indicators that any of these models were over-fitting in the cross validation. The argument could actually be made that the models are underfitting the data given the complexity of prediction at hand.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Feature Engineering [6 pts]\n",
    "\n",
    "Pick **at least three new features** based on your Milestone 1, Part 5, results. You may pick new ones or\n",
    "use the same ones you chose for Milestone 1. \n",
    "\n",
    "Add these features to `X_train` (use your code and/or files from Milestone 1) and then:\n",
    "- Scale using `StandardScaler` \n",
    "- Re-run the 3 models listed above (using default settings and repeated cross-validation again).\n",
    "- Report the **mean and standard deviation of CV MAE Scores**.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "#Feature Engineering 1: Ratio feature (Bath to Bed Ratio) \n",
    "\n",
    "#Train\n",
    "X_train_num['bath_to_bed_ratio'] = X_train_num['fullbathcnt'] * X_train_num['bedroomcnt']\n",
    "\n",
    "#Test\n",
    "X_test_num['bath_to_bed_ratio'] = X_test_num['fullbathcnt'] * X_test_num['bedroomcnt']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Engineeing 2: Square Feet & Bathroom Interaction\n",
    "\n",
    "#Train\n",
    "X_train_num['sqft_bath_interaction'] = X_train_num['calculatedfinishedsquarefeet'] * X_train_num['fullbathcnt']\n",
    "\n",
    "#Test\n",
    "X_test_num['sqft_bath_interaction'] = X_test_num['calculatedfinishedsquarefeet'] * X_test_num['fullbathcnt']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Engineering 3: Sqaure feet & Property Type Interaction\n",
    "\n",
    "#Train\n",
    "X_train_num['sqft_vs_type_average'] = X_train_num['calculatedfinishedsquarefeet'] / X_train_num['propertylandusetypeid'].map(X_train_num.groupby('propertylandusetypeid')['calculatedfinishedsquarefeet'].mean())\n",
    "\n",
    "#Test\n",
    "X_test_num['sqft_vs_type_average'] = X_test_num['calculatedfinishedsquarefeet'] / X_test_num['propertylandusetypeid'].map(X_test_num.groupby('propertylandusetypeid')['calculatedfinishedsquarefeet'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add new features to numeric_cols\n",
    "numeric_cols_list = list(numeric_cols)\n",
    "numeric_cols_list.extend(['bath_to_bed_ratio', 'sqft_bath_interaction', 'sqft_vs_type_average'])\n",
    "numeric_cols = pd.Index(numeric_cols_list)\n",
    "numeric_cols = numeric_cols.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New train and test sets with new features\n",
    "X_train_num_scaled2 = pd.DataFrame(\n",
    "    scaler.fit_transform(X_train_num),\n",
    "    columns=numeric_cols,\n",
    "    index=X_train.index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_num_scaled2 = pd.DataFrame(\n",
    "    scaler.transform(X_test_num),\n",
    "    columns=numeric_cols,\n",
    "    index=X_test.index\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running models Fujnction\n",
    "\n",
    "def evaluate_models(models, X_data, y_data, cv_folds=None):\n",
    "  \n",
    "    if cv_folds is None:\n",
    "        cv_folds = RepeatedKFold(\n",
    "            n_splits=5,\n",
    "            n_repeats=5,\n",
    "            random_state=random_state\n",
    "        )\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        neg_mae_scores = cross_val_score(\n",
    "            model,\n",
    "            X_data,\n",
    "            y_data,\n",
    "            scoring=\"neg_mean_absolute_error\",\n",
    "            cv=cv_folds,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        mae_scores = -neg_mae_scores\n",
    "        results[name] = {\n",
    "            'mae_scores': mae_scores,\n",
    "            'mean_mae': mae_scores.mean(),\n",
    "            'std_mae': mae_scores.std()\n",
    "        }\n",
    "        print(f\"{name:20s} -> MAE: {mae_scores.mean():.3f} ± {mae_scores.std():.3f}\")\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dict = {\n",
    "    \"Ridge Regression\": ridge,\n",
    "    \"Bagging Regressor\": bagging, \n",
    "    \"Gradient Boosting\": gbr\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression     -> MAE: 230466.011 ± 3225.957\n",
      "Bagging Regressor    -> MAE: 198443.023 ± 2403.191\n",
      "Gradient Boosting    -> MAE: 200116.990 ± 2430.064\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Ridge Regression': {'mae_scores': array([228778.52926018, 229568.7393022 , 231279.2329585 , 234894.02588305,\n",
       "         227817.93760318, 226357.00712834, 229929.53491438, 232975.60021808,\n",
       "         231052.30314423, 232352.91760562, 224211.64737391, 234652.01635845,\n",
       "         231220.55380712, 228012.30569697, 234129.36840192, 233121.64143302,\n",
       "         228078.26082543, 233090.70251096, 232020.05128207, 225849.92974243,\n",
       "         235313.78151528, 226976.04005409, 233745.2296898 , 231598.91481065,\n",
       "         224624.00410142]),\n",
       "  'mean_mae': np.float64(230466.01102485112),\n",
       "  'std_mae': np.float64(3225.9571177426187)},\n",
       " 'Bagging Regressor': {'mae_scores': array([195125.68679429, 199494.15177043, 202574.92063674, 199954.18850823,\n",
       "         195934.38170875, 194873.30671115, 199117.79629428, 200180.85788712,\n",
       "         199098.03883237, 202179.46128861, 194259.10465886, 202703.88728052,\n",
       "         199089.49394894, 195202.58894152, 201437.55797236, 198451.57091556,\n",
       "         196893.53078655, 200148.16012639, 199016.83000456, 197662.4887184 ,\n",
       "         199102.25208334, 196278.41373505, 199438.67295778, 196434.15683456,\n",
       "         196424.06684911]),\n",
       "  'mean_mae': np.float64(198443.02264981795),\n",
       "  'std_mae': np.float64(2403.1912835026105)},\n",
       " 'Gradient Boosting': {'mae_scores': array([199526.07581424, 198761.04536554, 200984.56194626, 203015.79329778,\n",
       "         197847.27260212, 197151.57922971, 200186.55072875, 200847.46952386,\n",
       "         201235.2176736 , 202007.68673226, 193875.76916213, 203991.5967665 ,\n",
       "         201028.40427554, 198544.76379372, 203477.6084366 , 202590.4980343 ,\n",
       "         198433.40623783, 201000.054508  , 201760.24350198, 198125.85925607,\n",
       "         201415.55311149, 197376.94704595, 202971.22383444, 200467.67138634,\n",
       "         196301.88882926]),\n",
       "  'mean_mae': np.float64(200116.98964377173),\n",
       "  'std_mae': np.float64(2430.063733550974)}}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_models(models_dict,X_train_num_scaled2, y_train )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Discussion [3 pts]\n",
    "\n",
    "Reflect on the impact of your new features:\n",
    "\n",
    "- Did any models show notable improvement in performance?\n",
    "\n",
    "- Which new features seemed to help — and in which models?\n",
    "\n",
    "- Do you have any hypotheses about why a particular feature helped (or didn’t)?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Your text here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Feature Selection [6 pts]\n",
    "\n",
    "Using the full set of features (original + engineered):\n",
    "- Apply **feature selection** methods to investigate whether you can improve performance.\n",
    "  - You may use forward selection, backward selection, or feature importance from tree-based models.\n",
    "- For each model, identify the **best-performing subset of features**.\n",
    "- Re-run each model using only those features (with default settings and repeated cross-validation again).\n",
    "- Report the **mean and standard deviation of CV MAE Scores**.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Forward Selection Function\n",
    "def forward_selection(X, y, max_features=5):\n",
    "    selected_features = []\n",
    "    remaining_features = list(X.columns)\n",
    "    \n",
    "    for _ in range(max_features):\n",
    "        best_score = -np.inf\n",
    "        best_feature = None\n",
    "        \n",
    "        for feature in remaining_features:\n",
    "            # Test adding this feature\n",
    "            test_features = selected_features + [feature]\n",
    "            X_subset = X[test_features]\n",
    "            \n",
    "            # Cross-validation score\n",
    "            model = LogisticRegression(random_state=42)\n",
    "            scores = cross_val_score(model, X_subset, y, cv=5, scoring='accuracy')\n",
    "            avg_score = scores.mean()\n",
    "            \n",
    "            if avg_score > best_score:\n",
    "                best_score = avg_score\n",
    "                best_feature = feature\n",
    "        \n",
    "        if best_feature:\n",
    "            selected_features.append(best_feature)\n",
    "            remaining_features.remove(best_feature)\n",
    "            print(f\"Added {best_feature}, Score: {best_score:.4f}\")\n",
    "    \n",
    "    return selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_split.py:2734: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  and (type_of_target(y, input_name=\"y\") in (\"binary\", \"multiclass\"))\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_split.py:784: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  type_of_target_y = type_of_target(y)\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\multiclass.py:213: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  y_type = type_of_target(y, input_name=\"y\")\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_response.py:203: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  target_type = type_of_target(classes)\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\multiclass.py:213: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  y_type = type_of_target(y, input_name=\"y\")\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_response.py:203: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  target_type = type_of_target(classes)\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\multiclass.py:213: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  y_type = type_of_target(y, input_name=\"y\")\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_response.py:203: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  target_type = type_of_target(classes)\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\multiclass.py:213: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  y_type = type_of_target(y, input_name=\"y\")\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_response.py:203: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  target_type = type_of_target(classes)\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\multiclass.py:213: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  y_type = type_of_target(y, input_name=\"y\")\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_response.py:203: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  target_type = type_of_target(classes)\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_split.py:2734: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  and (type_of_target(y, input_name=\"y\") in (\"binary\", \"multiclass\"))\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_split.py:784: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  type_of_target_y = type_of_target(y)\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\multiclass.py:213: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  y_type = type_of_target(y, input_name=\"y\")\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_response.py:203: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  target_type = type_of_target(classes)\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\multiclass.py:213: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  y_type = type_of_target(y, input_name=\"y\")\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_response.py:203: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  target_type = type_of_target(classes)\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\multiclass.py:213: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  y_type = type_of_target(y, input_name=\"y\")\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_response.py:203: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  target_type = type_of_target(classes)\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\multiclass.py:213: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  y_type = type_of_target(y, input_name=\"y\")\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_response.py:203: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  target_type = type_of_target(classes)\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\multiclass.py:213: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  y_type = type_of_target(y, input_name=\"y\")\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_response.py:203: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  target_type = type_of_target(classes)\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_split.py:2734: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  and (type_of_target(y, input_name=\"y\") in (\"binary\", \"multiclass\"))\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_split.py:784: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  type_of_target_y = type_of_target(y)\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\multiclass.py:213: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  y_type = type_of_target(y, input_name=\"y\")\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_response.py:203: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  target_type = type_of_target(classes)\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\multiclass.py:213: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  y_type = type_of_target(y, input_name=\"y\")\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_response.py:203: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  target_type = type_of_target(classes)\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\multiclass.py:213: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  y_type = type_of_target(y, input_name=\"y\")\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_response.py:203: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  target_type = type_of_target(classes)\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\multiclass.py:213: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  y_type = type_of_target(y, input_name=\"y\")\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_response.py:203: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  target_type = type_of_target(classes)\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\multiclass.py:213: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  y_type = type_of_target(y, input_name=\"y\")\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_response.py:203: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  target_type = type_of_target(classes)\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_split.py:2734: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  and (type_of_target(y, input_name=\"y\") in (\"binary\", \"multiclass\"))\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_split.py:784: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  type_of_target_y = type_of_target(y)\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\multiclass.py:213: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  y_type = type_of_target(y, input_name=\"y\")\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_response.py:203: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  target_type = type_of_target(classes)\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\multiclass.py:213: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  y_type = type_of_target(y, input_name=\"y\")\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_response.py:203: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  target_type = type_of_target(classes)\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\multiclass.py:213: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  y_type = type_of_target(y, input_name=\"y\")\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_response.py:203: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  target_type = type_of_target(classes)\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\multiclass.py:213: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  y_type = type_of_target(y, input_name=\"y\")\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_response.py:203: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  target_type = type_of_target(classes)\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\multiclass.py:213: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  y_type = type_of_target(y, input_name=\"y\")\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_response.py:203: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  target_type = type_of_target(classes)\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_split.py:2734: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  and (type_of_target(y, input_name=\"y\") in (\"binary\", \"multiclass\"))\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_split.py:784: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  type_of_target_y = type_of_target(y)\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\multiclass.py:213: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  y_type = type_of_target(y, input_name=\"y\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[71]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m forward_features = \u001b[43mforward_selection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_num_scaled2\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[70]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mforward_selection\u001b[39m\u001b[34m(X, y, max_features)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Cross-validation score\u001b[39;00m\n\u001b[32m     16\u001b[39m model = LogisticRegression(random_state=\u001b[32m42\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m scores = \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_subset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43maccuracy\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m avg_score = scores.mean()\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m avg_score > best_score:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py:677\u001b[39m, in \u001b[36mcross_val_score\u001b[39m\u001b[34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, error_score)\u001b[39m\n\u001b[32m    674\u001b[39m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[32m    675\u001b[39m scorer = check_scoring(estimator, scoring=scoring)\n\u001b[32m--> \u001b[39m\u001b[32m677\u001b[39m cv_results = \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    680\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    682\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mscore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    683\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    684\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    687\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    688\u001b[39m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    689\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    690\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[33m\"\u001b[39m\u001b[33mtest_score\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py:399\u001b[39m, in \u001b[36mcross_validate\u001b[39m\u001b[34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[39m\n\u001b[32m    396\u001b[39m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[32m    397\u001b[39m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[32m    398\u001b[39m parallel = Parallel(n_jobs=n_jobs, verbose=verbose, pre_dispatch=pre_dispatch)\n\u001b[32m--> \u001b[39m\u001b[32m399\u001b[39m results = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscore_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\n\u001b[32m    417\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    419\u001b[39m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[32m    421\u001b[39m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[32m    422\u001b[39m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[32m    423\u001b[39m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:1986\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1984\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._get_sequential_output(iterable)\n\u001b[32m   1985\u001b[39m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1986\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[32m   1988\u001b[39m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[32m   1989\u001b[39m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[32m   1990\u001b[39m \u001b[38;5;66;03m# reused, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[32m   1991\u001b[39m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[32m   1992\u001b[39m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[32m   1993\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:1914\u001b[39m, in \u001b[36mParallel._get_sequential_output\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1912\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_batches += \u001b[32m1\u001b[39m\n\u001b[32m   1913\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_tasks += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1914\u001b[39m res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1915\u001b[39m \u001b[38;5;28mself\u001b[39m.n_completed_tasks += \u001b[32m1\u001b[39m\n\u001b[32m   1916\u001b[39m \u001b[38;5;28mself\u001b[39m.print_progress()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\parallel.py:147\u001b[39m, in \u001b[36m_FuncWrapper.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(**config), warnings.catch_warnings():\n\u001b[32m    146\u001b[39m     warnings.filters = warning_filters\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py:859\u001b[39m, in \u001b[36m_fit_and_score\u001b[39m\u001b[34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[39m\n\u001b[32m    857\u001b[39m         estimator.fit(X_train, **fit_params)\n\u001b[32m    858\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m859\u001b[39m         \u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    861\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    862\u001b[39m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[32m    863\u001b[39m     fit_time = time.time() - start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1363\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1356\u001b[39m     estimator._validate_params()\n\u001b[32m   1358\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1359\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1360\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1361\u001b[39m     )\n\u001b[32m   1362\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1363\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:1376\u001b[39m, in \u001b[36mLogisticRegression.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m   1373\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1374\u001b[39m     n_threads = \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1376\u001b[39m fold_coefs_ = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1377\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1378\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1379\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1380\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpos_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1381\u001b[39m \u001b[43m        \u001b[49m\u001b[43mCs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mC_\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1382\u001b[39m \u001b[43m        \u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1383\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1384\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1385\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1386\u001b[39m \u001b[43m        \u001b[49m\u001b[43msolver\u001b[49m\u001b[43m=\u001b[49m\u001b[43msolver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1387\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1388\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1389\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1390\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1391\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1392\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcoef\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1393\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpenalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpenalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1394\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1395\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1396\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1397\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1398\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclasses_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1399\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1401\u001b[39m fold_coefs_, _, n_iter_ = \u001b[38;5;28mzip\u001b[39m(*fold_coefs_)\n\u001b[32m   1402\u001b[39m \u001b[38;5;28mself\u001b[39m.n_iter_ = np.asarray(n_iter_, dtype=np.int32)[:, \u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:1986\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1984\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._get_sequential_output(iterable)\n\u001b[32m   1985\u001b[39m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1986\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[32m   1988\u001b[39m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[32m   1989\u001b[39m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[32m   1990\u001b[39m \u001b[38;5;66;03m# reused, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[32m   1991\u001b[39m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[32m   1992\u001b[39m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[32m   1993\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:1914\u001b[39m, in \u001b[36mParallel._get_sequential_output\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1912\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_batches += \u001b[32m1\u001b[39m\n\u001b[32m   1913\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_tasks += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1914\u001b[39m res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1915\u001b[39m \u001b[38;5;28mself\u001b[39m.n_completed_tasks += \u001b[32m1\u001b[39m\n\u001b[32m   1916\u001b[39m \u001b[38;5;28mself\u001b[39m.print_progress()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\parallel.py:147\u001b[39m, in \u001b[36m_FuncWrapper.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(**config), warnings.catch_warnings():\n\u001b[32m    146\u001b[39m     warnings.filters = warning_filters\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:456\u001b[39m, in \u001b[36m_logistic_regression_path\u001b[39m\u001b[34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio, n_threads)\u001b[39m\n\u001b[32m    452\u001b[39m l2_reg_strength = \u001b[32m1.0\u001b[39m / (C * sw_sum)\n\u001b[32m    453\u001b[39m iprint = [-\u001b[32m1\u001b[39m, \u001b[32m50\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m100\u001b[39m, \u001b[32m101\u001b[39m][\n\u001b[32m    454\u001b[39m     np.searchsorted(np.array([\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m]), verbose)\n\u001b[32m    455\u001b[39m ]\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m opt_res = \u001b[43moptimize\u001b[49m\u001b[43m.\u001b[49m\u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    458\u001b[39m \u001b[43m    \u001b[49m\u001b[43mw0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mL-BFGS-B\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml2_reg_strength\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaxiter\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaxls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# default is 20\u001b[39;49;00m\n\u001b[32m    465\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43miprint\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43miprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgtol\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mftol\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    470\u001b[39m n_iter_i = _check_optimize_result(\n\u001b[32m    471\u001b[39m     solver,\n\u001b[32m    472\u001b[39m     opt_res,\n\u001b[32m    473\u001b[39m     max_iter,\n\u001b[32m    474\u001b[39m     extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n\u001b[32m    475\u001b[39m )\n\u001b[32m    476\u001b[39m w0, loss = opt_res.x, opt_res.fun\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\scipy\\optimize\\_minimize.py:785\u001b[39m, in \u001b[36mminimize\u001b[39m\u001b[34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[39m\n\u001b[32m    782\u001b[39m     res = _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[32m    783\u001b[39m                              **options)\n\u001b[32m    784\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m meth == \u001b[33m'\u001b[39m\u001b[33ml-bfgs-b\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m785\u001b[39m     res = \u001b[43m_minimize_lbfgsb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    786\u001b[39m \u001b[43m                           \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m meth == \u001b[33m'\u001b[39m\u001b[33mtnc\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    788\u001b[39m     res = _minimize_tnc(fun, x0, args, jac, bounds, callback=callback,\n\u001b[32m    789\u001b[39m                         **options)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\scipy\\optimize\\_lbfgsb_py.py:469\u001b[39m, in \u001b[36m_minimize_lbfgsb\u001b[39m\u001b[34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, workers, **unknown_options)\u001b[39m\n\u001b[32m    461\u001b[39m _lbfgsb.setulb(m, x, low_bnd, upper_bnd, nbd, f, g, factr, pgtol, wa,\n\u001b[32m    462\u001b[39m                iwa, task, lsave, isave, dsave, maxls, ln_task)\n\u001b[32m    464\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m task[\u001b[32m0\u001b[39m] == \u001b[32m3\u001b[39m:\n\u001b[32m    465\u001b[39m     \u001b[38;5;66;03m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[32m    466\u001b[39m     \u001b[38;5;66;03m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[32m    467\u001b[39m     \u001b[38;5;66;03m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[32m    468\u001b[39m     \u001b[38;5;66;03m# Overwrite f and g:\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m469\u001b[39m     f, g = \u001b[43mfunc_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m task[\u001b[32m0\u001b[39m] == \u001b[32m1\u001b[39m:\n\u001b[32m    471\u001b[39m     \u001b[38;5;66;03m# new iteration\u001b[39;00m\n\u001b[32m    472\u001b[39m     n_iterations += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\scipy\\optimize\\_differentiable_functions.py:403\u001b[39m, in \u001b[36mScalarFunction.fun_and_grad\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.array_equal(x, \u001b[38;5;28mself\u001b[39m.x):\n\u001b[32m    402\u001b[39m     \u001b[38;5;28mself\u001b[39m._update_x(x)\n\u001b[32m--> \u001b[39m\u001b[32m403\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[38;5;28mself\u001b[39m._update_grad()\n\u001b[32m    405\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.f, \u001b[38;5;28mself\u001b[39m.g\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\scipy\\optimize\\_differentiable_functions.py:353\u001b[39m, in \u001b[36mScalarFunction._update_fun\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    351\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    352\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.f_updated:\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m         fx = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_wrapped_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    354\u001b[39m         \u001b[38;5;28mself\u001b[39m._nfev += \u001b[32m1\u001b[39m\n\u001b[32m    355\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m fx < \u001b[38;5;28mself\u001b[39m._lowest_f:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\scipy\\_lib\\_util.py:583\u001b[39m, in \u001b[36m_ScalarFunctionWrapper.__call__\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    580\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m    581\u001b[39m     \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[32m    582\u001b[39m     \u001b[38;5;66;03m# The user of this class might want `x` to remain unchanged.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m583\u001b[39m     fx = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    584\u001b[39m     \u001b[38;5;28mself\u001b[39m.nfev += \u001b[32m1\u001b[39m\n\u001b[32m    586\u001b[39m     \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\scipy\\optimize\\_optimize.py:80\u001b[39m, in \u001b[36mMemoizeJac.__call__\u001b[39m\u001b[34m(self, x, *args)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, *args):\n\u001b[32m     79\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\" returns the function value \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_compute_if_needed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\scipy\\optimize\\_optimize.py:74\u001b[39m, in \u001b[36mMemoizeJac._compute_if_needed\u001b[39m\u001b[34m(self, x, *args)\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.all(x == \u001b[38;5;28mself\u001b[39m.x) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.jac \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     73\u001b[39m     \u001b[38;5;28mself\u001b[39m.x = np.asarray(x).copy()\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     fg = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28mself\u001b[39m.jac = fg[\u001b[32m1\u001b[39m]\n\u001b[32m     76\u001b[39m     \u001b[38;5;28mself\u001b[39m._value = fg[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_linear_loss.py:316\u001b[39m, in \u001b[36mLinearModelLoss.loss_gradient\u001b[39m\u001b[34m(self, coef, X, y, sample_weight, l2_reg_strength, n_threads, raw_prediction)\u001b[39m\n\u001b[32m    313\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    314\u001b[39m     weights, intercept = \u001b[38;5;28mself\u001b[39m.weight_intercept(coef)\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m loss, grad_pointwise = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_loss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloss_gradient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m    \u001b[49m\u001b[43mraw_prediction\u001b[49m\u001b[43m=\u001b[49m\u001b[43mraw_prediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    322\u001b[39m sw_sum = n_samples \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m np.sum(sample_weight)\n\u001b[32m    323\u001b[39m loss = loss.sum() / sw_sum\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\_loss\\loss.py:205\u001b[39m, in \u001b[36mBaseLoss.loss_gradient\u001b[39m\u001b[34m(self, y_true, raw_prediction, sample_weight, loss_out, gradient_out, n_threads)\u001b[39m\n\u001b[32m    196\u001b[39m     \u001b[38;5;28mself\u001b[39m.closs.loss(\n\u001b[32m    197\u001b[39m         y_true=y_true,\n\u001b[32m    198\u001b[39m         raw_prediction=raw_prediction,\n\u001b[32m   (...)\u001b[39m\u001b[32m    201\u001b[39m         n_threads=n_threads,\n\u001b[32m    202\u001b[39m     )\n\u001b[32m    203\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_out\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mloss_gradient\u001b[39m(\n\u001b[32m    206\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    207\u001b[39m     y_true,\n\u001b[32m    208\u001b[39m     raw_prediction,\n\u001b[32m    209\u001b[39m     sample_weight=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    210\u001b[39m     loss_out=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    211\u001b[39m     gradient_out=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    212\u001b[39m     n_threads=\u001b[32m1\u001b[39m,\n\u001b[32m    213\u001b[39m ):\n\u001b[32m    214\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Compute loss and gradient w.r.t. raw_prediction for each input.\u001b[39;00m\n\u001b[32m    215\u001b[39m \n\u001b[32m    216\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    241\u001b[39m \u001b[33;03m        Element-wise gradients.\u001b[39;00m\n\u001b[32m    242\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m loss_out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "forward_features = forward_selection(X_train_num_scaled2,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backward Selection Function\n",
    "def backward_elimination(X, y, min_features=5):\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    \n",
    "    features = list(X.columns)  # Start with all features\n",
    "    \n",
    "    while len(features) > min_features:\n",
    "        best_to_remove = None\n",
    "        smallest_drop = float('inf')\n",
    "        \n",
    "        # Test removing each feature\n",
    "        for feature in features:\n",
    "            # Try without this feature\n",
    "            remaining = [f for f in features if f != feature]\n",
    "            score = cross_val_score(LogisticRegression(random_state=42), \n",
    "                                  X[remaining], y, cv=5).mean()\n",
    "            \n",
    "            # Current score with all features\n",
    "            current_score = cross_val_score(LogisticRegression(random_state=42), \n",
    "                                          X[features], y, cv=5).mean()\n",
    "            \n",
    "            drop = current_score - score  # How much we lose\n",
    "            \n",
    "            if drop < smallest_drop:  # Find least harmful\n",
    "                smallest_drop = drop\n",
    "                best_to_remove = feature\n",
    "        \n",
    "        # Remove the least harmful feature\n",
    "        features.remove(best_to_remove)\n",
    "        print(f\"Removed {best_to_remove}\")\n",
    "    \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backward_features = backward_elimination(X_train_num_scaled2,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-Run Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_model_performance = evaluate_models(models_dict,X_train_num_scaled2[forward_features], y_train[forward_features] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backward_model_performance = evaluate_models(models_dict,X_train_num_scaled2[backward_features], y_train[backward_features] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Discussion [3 pts]\n",
    "\n",
    "Analyze the effect of feature selection on your models:\n",
    "\n",
    "- Did performance improve for any models after reducing the number of features?\n",
    "\n",
    "- Which features were consistently retained across models?\n",
    "\n",
    "- Were any of your newly engineered features selected as important?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Your text here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Fine-Tuning Your Three Models [6 pts]\n",
    "\n",
    "In this final phase of Milestone 2, you’ll select and refine your **three most promising models and their corresponding data pipelines** based on everything you've done so far, and pick a winner!\n",
    "\n",
    "1. For each of your three models:\n",
    "    - Choose your best engineered features and best selection of features as determined above. \n",
    "   - Perform hyperparameter tuning using `sweep_parameters`, `GridSearchCV`, `RandomizedSearchCV`, `Optuna`, etc. as you have practiced in previous homeworks. \n",
    "3. Decide on the best hyperparameters for each model, and for each run with repeated CV and record their final results:\n",
    "    - Report the **mean and standard deviation of CV MAE Score**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add as many cells as you need\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Discussion [3 pts]\n",
    "\n",
    "Reflect on your tuning process and final results:\n",
    "\n",
    "- What was your tuning strategy for each model? Why did you choose those hyperparameters?\n",
    "- Did you find that certain types of preprocessing or feature engineering worked better with specific models?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Your text here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5: Final Model and Design Reassessment [6 pts]\n",
    "\n",
    "In this part, you will finalize your best-performing model.  You’ll also consolidate and present the key code used to run your model on the preprocessed dataset.\n",
    "**Requirements:**\n",
    "\n",
    "- Decide one your final model among the three contestants. \n",
    "\n",
    "- Below, include all code necessary to **run your final model** on the processed dataset, reporting\n",
    "\n",
    "    - Mean and standard deviation of CV MAE Score.\n",
    "    \n",
    "    - Test score on held-out test set. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add as many cells as you need\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5: Discussion [8 pts]\n",
    "\n",
    "In this final step, your goal is to synthesize your entire modeling process and assess how your earlier decisions influenced the outcome. Please address the following:\n",
    "\n",
    "1. Model Selection:\n",
    "- Clearly state which model you selected as your final model and why.\n",
    "\n",
    "- What metrics or observations led you to this decision?\n",
    "\n",
    "- Were there trade-offs (e.g., interpretability vs. performance) that influenced your choice?\n",
    "\n",
    "2. Revisiting an Early Decision\n",
    "\n",
    "- Identify one specific preprocessing or feature engineering decision from Milestone 1 (e.g., how you handled missing values, how you scaled or encoded a variable, or whether you created interaction or polynomial terms).\n",
    "\n",
    "- Explain the rationale for that decision at the time: What were you hoping it would achieve?\n",
    "\n",
    "- Now that you've seen the full modeling pipeline and final results, reflect on whether this step helped or hindered performance. Did you keep it, modify it, or remove it?\n",
    "\n",
    "- Justify your final decision with evidence—such as validation scores, visualizations, or model diagnostics.\n",
    "\n",
    "3. Lessons Learned\n",
    "\n",
    "- What insights did you gain about your dataset or your modeling process through this end-to-end workflow?\n",
    "\n",
    "- If you had more time or data, what would you explore next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Your text here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
