{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Milestone Two: Modeling and Feature Engineering\n",
    "\n",
    "### Due: Midnight on August 3 (with 2-hour grace period) and worth 50 points\n",
    "\n",
    "### Overview\n",
    "\n",
    "This milestone builds on your work from Milestone 1 and will complete the coding portion of your project. You will:\n",
    "\n",
    "1. Pick 3 modeling algorithms from those we have studied.\n",
    "2. Evaluate baseline models using default settings.\n",
    "3. Engineer new features and re-evaluate models.\n",
    "4. Use feature selection techniques and re-evaluate.\n",
    "5. Fine-tune for optimal performance.\n",
    "6. Select your best model and report on your results. \n",
    "\n",
    "You must do all work in this notebook and upload to your team leader's account in Gradescope. There is no\n",
    "Individual Assessment for this Milestone. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# Useful Imports: Add more as needed\n",
    "# ===================================\n",
    "\n",
    "# Standard Libraries\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "from itertools import chain, combinations\n",
    "\n",
    "# Data Science Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.ticker as mticker  # Optional: Format y-axis labels as dollars\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn (Machine Learning)\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, \n",
    "    cross_val_score, \n",
    "    GridSearchCV, \n",
    "    RandomizedSearchCV, \n",
    "    RepeatedKFold\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_selection import SequentialFeatureSelector, f_regression, SelectKBest\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "# Progress Tracking\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =============================\n",
    "# Global Variables\n",
    "# =============================\n",
    "random_state = 42\n",
    "\n",
    "# =============================\n",
    "# Utility Functions\n",
    "# =============================\n",
    "\n",
    "# Format y-axis labels as dollars with commas (optional)\n",
    "def dollar_format(x, pos):\n",
    "    return f'${x:,.0f}'\n",
    "\n",
    "# Convert seconds to HH:MM:SS format\n",
    "def format_hms(seconds):\n",
    "    return time.strftime(\"%H:%M:%S\", time.gmtime(seconds))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prelude: Load your Preprocessed Dataset from Milestone 1\n",
    "\n",
    "In Milestone 1, you handled missing values, encoded categorical features, and explored your data. Before you begin this milestone, you’ll need to load that cleaned dataset and prepare it for modeling. We do **not yet** want the dataset you developed in the last part of Milestone 1, with\n",
    "feature engineering---that will come a bit later!\n",
    "\n",
    "Here’s what to do:\n",
    "\n",
    "1. Return to your Milestone 1 notebook and rerun your code through Part 3, where your dataset was fully cleaned (assume it’s called `df_cleaned`).\n",
    "\n",
    "2. **Save** the cleaned dataset to a file by running:\n",
    "\n",
    ">   df_cleaned.to_csv(\"zillow_cleaned.csv\", index=False)\n",
    "\n",
    "3. Switch to this notebook and **load** the saved data:\n",
    "\n",
    ">   df = pd.read_csv(\"zillow_cleaned.csv\")\n",
    "\n",
    "4. Create a **train/test split** using `train_test_split`.  \n",
    "   \n",
    "6. **Standardize** the features (but not the target!) using **only the training data.** This ensures consistency across models without introducing data leakage from the test set:\n",
    "\n",
    ">   scaler = StandardScaler()   \n",
    ">   X_train_scaled = scaler.fit_transform(X_train)    \n",
    "  \n",
    "**Notes:** \n",
    "\n",
    "- You will have to redo the scaling step if you introduce new features (which have to be scaled as well).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add as many cells as you need\n",
    "\n",
    "# Step 1-3\n",
    "df = pd.read_csv(\"zillow_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62060, 22) (15516, 22) (62060,) (15516,)\n"
     ]
    }
   ],
   "source": [
    "# Step 4\n",
    "X = df.drop(columns=[\"taxvaluedollarcnt\"])   # all features\n",
    "y = df[\"taxvaluedollarcnt\"]                  # target\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=random_state\n",
    ")\n",
    "\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62060, 22) (15516, 22)\n"
     ]
    }
   ],
   "source": [
    "# 5a) Numeric vs. categorical columns\n",
    "numeric_cols     = X_train.select_dtypes(include=[np.number]).columns\n",
    "categorical_cols = X_train.select_dtypes(exclude=[np.number]).columns\n",
    "\n",
    "# 5b) Median for numerics, most frequent for categoricals\n",
    "num_imp = SimpleImputer(strategy=\"median\")\n",
    "cat_imp = SimpleImputer(strategy=\"most_frequent\")\n",
    "\n",
    "X_train_num = pd.DataFrame(\n",
    "    num_imp.fit_transform(X_train[numeric_cols]),\n",
    "    columns=numeric_cols,\n",
    "    index=X_train.index\n",
    ")\n",
    "X_test_num  = pd.DataFrame(\n",
    "    num_imp.transform(X_test[numeric_cols]),\n",
    "    columns=numeric_cols,\n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "X_train_cat = pd.DataFrame(\n",
    "    cat_imp.fit_transform(X_train[categorical_cols]),\n",
    "    columns=categorical_cols,\n",
    "    index=X_train.index\n",
    ")\n",
    "X_test_cat  = pd.DataFrame(\n",
    "    cat_imp.transform(X_test[categorical_cols]),\n",
    "    columns=categorical_cols,\n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "# 5c) Scale the numeric block\n",
    "scaler = StandardScaler()\n",
    "X_train_num_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(X_train_num),\n",
    "    columns=numeric_cols,\n",
    "    index=X_train.index\n",
    ")\n",
    "X_test_num_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_test_num),\n",
    "    columns=numeric_cols,\n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "# 5d) Ordinal‐encode to (now imputed) categoricals,\n",
    "#   \n",
    "encoder = OrdinalEncoder(\n",
    "    handle_unknown=\"use_encoded_value\",\n",
    "    unknown_value=-1\n",
    ")\n",
    "X_train_cat_enc = pd.DataFrame(\n",
    "    encoder.fit_transform(X_train_cat),\n",
    "    columns=categorical_cols,\n",
    "    index=X_train.index\n",
    ")\n",
    "X_test_cat_enc  = pd.DataFrame(\n",
    "    encoder.transform(X_test_cat),\n",
    "    columns=categorical_cols,\n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "# 5e) Final\n",
    "X_train_scaled = pd.concat([X_train_num_scaled, X_train_cat_enc], axis=1)\n",
    "X_test_scaled  = pd.concat([X_test_num_scaled, X_test_cat_enc],   axis=1)\n",
    "\n",
    "print(X_train_scaled.shape, X_test_scaled.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Picking Three Models and Establishing Baselines [6 pts]\n",
    "\n",
    "Apply the following regression models to the scaled training dataset using **default parameters** for **three** of the models we have worked with this term:\n",
    "\n",
    "- Linear Regression\n",
    "- Ridge Regression\n",
    "- Lasso Regression\n",
    "- Decision Tree Regression\n",
    "- Bagging\n",
    "- Random Forest\n",
    "- Gradient Boosting Trees\n",
    "\n",
    "For each of the three models:\n",
    "- Use **repeated cross-validation** (e.g., 5 folds, 5 repeats).\n",
    "- Report the **mean and standard deviation of CV MAE Score**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression     → MAE: 243773 ± 3343\n",
      "Bagging Regressor    → MAE: 198207 ± 2761\n",
      "Gradient Boosting    → MAE: 200104 ± 2555\n"
     ]
    }
   ],
   "source": [
    "# 3 Models are ridge, bagging, gbr\n",
    "ridge = Ridge(random_state=random_state)\n",
    "bagging = BaggingRegressor(random_state=random_state)\n",
    "gbr = GradientBoostingRegressor(random_state=random_state)\n",
    "\n",
    "models = [\n",
    "    (\"Ridge Regression\", ridge),\n",
    "    (\"Bagging Regressor\", bagging),\n",
    "    (\"Gradient Boosting\", gbr),\n",
    "]\n",
    "\n",
    "# Repeated CV (5 folds x 5 repeats)\n",
    "rkf = RepeatedKFold(\n",
    "    n_splits=5,\n",
    "    n_repeats=5,\n",
    "    random_state=random_state\n",
    ")\n",
    "\n",
    "# Evaluate each model\n",
    "for name, model in models:\n",
    "    neg_mae_scores = cross_val_score(\n",
    "        model,\n",
    "        X_train_scaled,\n",
    "        y_train,\n",
    "        scoring=\"neg_mean_absolute_error\",\n",
    "        cv=rkf,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    mae_scores = -neg_mae_scores  \n",
    "    print(f\"{name:20s} → MAE: {mae_scores.mean():.0f} ± {mae_scores.std():.0f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest → MAE: 190862 ± 2583\n"
     ]
    }
   ],
   "source": [
    "# Added Random Forest to test (all had high MAE)\n",
    "rf = RandomForestRegressor(random_state=random_state)\n",
    "\n",
    "neg_mae = cross_val_score(\n",
    "    rf,\n",
    "    X_train_scaled,     \n",
    "    y_train,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    cv=rkf,              \n",
    "    n_jobs=-1\n",
    ")\n",
    "mae = -neg_mae\n",
    "print(f\"Random Forest → MAE: {mae.mean():.0f} ± {mae.std():.0f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Bar Chart to compare / evaluate \n",
    "\n",
    "models = [\"Ridge\", \"Bagging\", \"GBM\", \"RF\"]\n",
    "means  = [244000, 198000, 200000, 191000]  # your exact means\n",
    "stds   = [3300,   2760,   2555,   2583]    # your exact stds\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar(models, means, yerr=stds, capsize=5)\n",
    "plt.ylabel(\"Mean MAE\")\n",
    "plt.title(\"Model Comparison: Mean ±1o MAE\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Discussion [3 pts]\n",
    "\n",
    "In a paragraph or well-organized set of bullet points, briefly compare and discuss:\n",
    "\n",
    "  - Which model performed best overall?\n",
    "  - Which was most stable (lowest std)?\n",
    "  - Any signs of overfitting or underfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random Forest Model was the best overall performer in terms of mean absolute error with roughly 191,000 and it had the smallest standard deviation with 2,600. The worst performer was the Ridge model with approximately 244,000 in terms of MAE and was the least stable at 3,300 standard deviation. This makes sense because of the numerous complex variables used in predicting house prices that it would not be simple enought to fit a linear model like the ridge for this situation. I decided to try a fourth model with the random forest because the MAE were all very large for the first three models selected and I wanted to explore more. While the random forest helped improve the MAE there were no large indicators that any of these models were over-fitting in the cross validation. The argument could actually be made that the models are underfitting the data given the complexity of prediction at hand.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Feature Engineering [6 pts]\n",
    "\n",
    "Pick **at least three new features** based on your Milestone 1, Part 5, results. You may pick new ones or\n",
    "use the same ones you chose for Milestone 1. \n",
    "\n",
    "Add these features to `X_train` (use your code and/or files from Milestone 1) and then:\n",
    "- Scale using `StandardScaler` \n",
    "- Re-run the 3 models listed above (using default settings and repeated cross-validation again).\n",
    "- Report the **mean and standard deviation of CV MAE Scores**.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "#Feature Engineering 1: Ratio feature (Bath to Bed Ratio) \n",
    "\n",
    "#Train\n",
    "X_train_num['bath_to_bed_ratio'] = X_train_num['fullbathcnt'] * X_train_num['bedroomcnt']\n",
    "\n",
    "#Test\n",
    "X_test_num['bath_to_bed_ratio'] = X_test_num['fullbathcnt'] * X_test_num['bedroomcnt']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Engineeing 2: Square Feet & Bathroom Interaction\n",
    "\n",
    "#Train\n",
    "X_train_num['sqft_bath_interaction'] = X_train_num['calculatedfinishedsquarefeet'] * X_train_num['fullbathcnt']\n",
    "\n",
    "#Test\n",
    "X_test_num['sqft_bath_interaction'] = X_test_num['calculatedfinishedsquarefeet'] * X_test_num['fullbathcnt']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Engineering 3: Sqaure feet & Property Type Interaction\n",
    "\n",
    "#Train\n",
    "X_train_num['sqft_vs_type_average'] = X_train_num['calculatedfinishedsquarefeet'] / X_train_num['propertylandusetypeid'].map(X_train_num.groupby('propertylandusetypeid')['calculatedfinishedsquarefeet'].mean())\n",
    "\n",
    "#Test\n",
    "X_test_num['sqft_vs_type_average'] = X_test_num['calculatedfinishedsquarefeet'] / X_test_num['propertylandusetypeid'].map(X_test_num.groupby('propertylandusetypeid')['calculatedfinishedsquarefeet'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add new features to numeric_cols\n",
    "numeric_cols_list = list(numeric_cols)\n",
    "numeric_cols_list.extend(['bath_to_bed_ratio', 'sqft_bath_interaction', 'sqft_vs_type_average'])\n",
    "numeric_cols = pd.Index(numeric_cols_list)\n",
    "numeric_cols = numeric_cols.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New train and test sets with new features\n",
    "X_train_num_scaled2 = pd.DataFrame(\n",
    "    scaler.fit_transform(X_train_num),\n",
    "    columns=numeric_cols,\n",
    "    index=X_train.index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_num_scaled2 = pd.DataFrame(\n",
    "    scaler.transform(X_test_num),\n",
    "    columns=numeric_cols,\n",
    "    index=X_test.index\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running models Function\n",
    "\n",
    "def evaluate_models(models, X_data, y_data, cv_folds=None):\n",
    "  \n",
    "    if cv_folds is None:\n",
    "        cv_folds = RepeatedKFold(\n",
    "            n_splits=5,\n",
    "            n_repeats=5,\n",
    "            random_state=random_state\n",
    "        )\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        neg_mae_scores = cross_val_score(\n",
    "            model,\n",
    "            X_data,\n",
    "            y_data,\n",
    "            scoring=\"neg_mean_absolute_error\",\n",
    "            cv=cv_folds,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        mae_scores = -neg_mae_scores\n",
    "        results[name] = {\n",
    "            'mae_scores': mae_scores,\n",
    "            'mean_mae': mae_scores.mean(),\n",
    "            'std_mae': mae_scores.std()\n",
    "        }\n",
    "        print(f\"{name:20s} -> MAE: {mae_scores.mean():.3f} ± {mae_scores.std():.3f}\")\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dict = {\n",
    "    \"Ridge Regression\": ridge,\n",
    "    \"Bagging Regressor\": bagging, \n",
    "    \"Gradient Boosting\": gbr\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression     -> MAE: 230466.011 ± 3225.957\n",
      "Bagging Regressor    -> MAE: 198443.023 ± 2403.191\n",
      "Gradient Boosting    -> MAE: 200116.990 ± 2430.064\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Ridge Regression': {'mae_scores': array([228778.52926018, 229568.7393022 , 231279.2329585 , 234894.02588305,\n",
       "         227817.93760318, 226357.00712834, 229929.53491438, 232975.60021808,\n",
       "         231052.30314423, 232352.91760562, 224211.64737391, 234652.01635845,\n",
       "         231220.55380712, 228012.30569697, 234129.36840192, 233121.64143302,\n",
       "         228078.26082543, 233090.70251096, 232020.05128207, 225849.92974243,\n",
       "         235313.78151528, 226976.04005409, 233745.2296898 , 231598.91481065,\n",
       "         224624.00410142]),\n",
       "  'mean_mae': np.float64(230466.01102485112),\n",
       "  'std_mae': np.float64(3225.9571177426187)},\n",
       " 'Bagging Regressor': {'mae_scores': array([195125.68679429, 199494.15177043, 202574.92063674, 199954.18850823,\n",
       "         195934.38170875, 194873.30671115, 199117.79629428, 200180.85788712,\n",
       "         199098.03883237, 202179.46128861, 194259.10465886, 202703.88728052,\n",
       "         199089.49394894, 195202.58894152, 201437.55797236, 198451.57091556,\n",
       "         196893.53078655, 200148.16012639, 199016.83000456, 197662.4887184 ,\n",
       "         199102.25208334, 196278.41373505, 199438.67295778, 196434.15683456,\n",
       "         196424.06684911]),\n",
       "  'mean_mae': np.float64(198443.02264981795),\n",
       "  'std_mae': np.float64(2403.1912835026105)},\n",
       " 'Gradient Boosting': {'mae_scores': array([199526.07581424, 198761.04536554, 200984.56194626, 203015.79329778,\n",
       "         197847.27260212, 197151.57922971, 200186.55072875, 200847.46952386,\n",
       "         201235.2176736 , 202007.68673226, 193875.76916213, 203991.5967665 ,\n",
       "         201028.40427554, 198544.76379372, 203477.6084366 , 202590.4980343 ,\n",
       "         198433.40623783, 201000.054508  , 201760.24350198, 198125.85925607,\n",
       "         201415.55311149, 197376.94704595, 202971.22383444, 200467.67138634,\n",
       "         196301.88882926]),\n",
       "  'mean_mae': np.float64(200116.98964377173),\n",
       "  'std_mae': np.float64(2430.063733550974)}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_models(models_dict,X_train_num_scaled2, y_train )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Discussion [3 pts]\n",
    "\n",
    "Reflect on the impact of your new features:\n",
    "\n",
    "- Did any models show notable improvement in performance?\n",
    "\n",
    "- Which new features seemed to help — and in which models?\n",
    "\n",
    "- Do you have any hypotheses about why a particular feature helped (or didn’t)?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The added features showed great improvement  for the Ridge Regression model (~13000 improvement). Almost no chage occured in the baggins regressor and gradient boosting models. Typically, ridge regression struggles to capture complex, non-linear relationships. By engineering new features that encapsulate this complexity, the model is easier able to improve. Ensemble method like random forest baggin and gradient boosting are already more equipped to uncover non-linear relationships, thus the new features did not yield better results.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Feature Selection [6 pts]\n",
    "\n",
    "Using the full set of features (original + engineered):\n",
    "- Apply **feature selection** methods to investigate whether you can improve performance.\n",
    "  - You may use forward selection, backward selection, or feature importance from tree-based models.\n",
    "- For each model, identify the **best-performing subset of features**.\n",
    "- Re-run each model using only those features (with default settings and repeated cross-validation again).\n",
    "- Report the **mean and standard deviation of CV MAE Scores**.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Forward Selection Function\n",
    "def forward_selection(X, y, max_features=5):\n",
    "    # Use statistical test to rank all features at once\n",
    "    selector = SelectKBest(score_func=f_classif, k=max_features)\n",
    "    selector.fit(X, y)\n",
    "    return selector.get_support(indices=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sample = X_train_num_scaled2.sample(n=10000, random_state=42)\n",
    "y_sample = y_train[X_sample.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:110: UserWarning: Features [12] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:111: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>calculatedfinishedsquarefeet</th>\n",
       "      <th>propertylandusetypeid</th>\n",
       "      <th>regionidzip</th>\n",
       "      <th>sqft_bath_interaction</th>\n",
       "      <th>sqft_vs_type_average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37954</th>\n",
       "      <td>-0.303968</td>\n",
       "      <td>-0.151425</td>\n",
       "      <td>-0.031985</td>\n",
       "      <td>-0.313422</td>\n",
       "      <td>-0.468972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32219</th>\n",
       "      <td>0.702361</td>\n",
       "      <td>-0.151425</td>\n",
       "      <td>-0.120623</td>\n",
       "      <td>0.918034</td>\n",
       "      <td>0.581085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14684</th>\n",
       "      <td>-0.431723</td>\n",
       "      <td>-0.151425</td>\n",
       "      <td>-0.099949</td>\n",
       "      <td>-0.604893</td>\n",
       "      <td>-0.602277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19698</th>\n",
       "      <td>1.247937</td>\n",
       "      <td>-0.151425</td>\n",
       "      <td>-0.055501</td>\n",
       "      <td>0.756986</td>\n",
       "      <td>1.150366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50296</th>\n",
       "      <td>0.624871</td>\n",
       "      <td>-0.151425</td>\n",
       "      <td>0.126684</td>\n",
       "      <td>0.435431</td>\n",
       "      <td>0.500227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51986</th>\n",
       "      <td>-0.360515</td>\n",
       "      <td>0.789484</td>\n",
       "      <td>-0.056793</td>\n",
       "      <td>-0.073112</td>\n",
       "      <td>0.251317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>-0.622308</td>\n",
       "      <td>-0.151425</td>\n",
       "      <td>-0.140262</td>\n",
       "      <td>-0.422949</td>\n",
       "      <td>-0.801144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66796</th>\n",
       "      <td>-0.488270</td>\n",
       "      <td>-0.151425</td>\n",
       "      <td>-0.016738</td>\n",
       "      <td>-0.376833</td>\n",
       "      <td>-0.661282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17601</th>\n",
       "      <td>-0.777289</td>\n",
       "      <td>-0.151425</td>\n",
       "      <td>-0.102016</td>\n",
       "      <td>-0.664341</td>\n",
       "      <td>-0.962859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73885</th>\n",
       "      <td>-0.853732</td>\n",
       "      <td>0.789484</td>\n",
       "      <td>0.104719</td>\n",
       "      <td>-0.502572</td>\n",
       "      <td>-0.517871</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       calculatedfinishedsquarefeet  propertylandusetypeid  regionidzip  \\\n",
       "37954                     -0.303968              -0.151425    -0.031985   \n",
       "32219                      0.702361              -0.151425    -0.120623   \n",
       "14684                     -0.431723              -0.151425    -0.099949   \n",
       "19698                      1.247937              -0.151425    -0.055501   \n",
       "50296                      0.624871              -0.151425     0.126684   \n",
       "...                             ...                    ...          ...   \n",
       "51986                     -0.360515               0.789484    -0.056793   \n",
       "954                       -0.622308              -0.151425    -0.140262   \n",
       "66796                     -0.488270              -0.151425    -0.016738   \n",
       "17601                     -0.777289              -0.151425    -0.102016   \n",
       "73885                     -0.853732               0.789484     0.104719   \n",
       "\n",
       "       sqft_bath_interaction  sqft_vs_type_average  \n",
       "37954              -0.313422             -0.468972  \n",
       "32219               0.918034              0.581085  \n",
       "14684              -0.604893             -0.602277  \n",
       "19698               0.756986              1.150366  \n",
       "50296               0.435431              0.500227  \n",
       "...                      ...                   ...  \n",
       "51986              -0.073112              0.251317  \n",
       "954                -0.422949             -0.801144  \n",
       "66796              -0.376833             -0.661282  \n",
       "17601              -0.664341             -0.962859  \n",
       "73885              -0.502572             -0.517871  \n",
       "\n",
       "[10000 rows x 5 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward_selected_indices = forward_selection(X_sample, y_sample, max_features=5)\n",
    "forward_selected_data = X_sample.iloc[:, forward_selected_indices]  # For pandas DataFrames\n",
    "forward_selected_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backward Selection Function\n",
    "def fast_backward_elimination(X, y, min_features=5):\n",
    "    # Get all feature scores at once\n",
    "    selector = SelectKBest(score_func=f_classif, k='all')\n",
    "    selector.fit(X, y)\n",
    "    scores = selector.scores_\n",
    "    \n",
    "    # Start with all features, remove lowest scoring ones\n",
    "    n_features = X.shape[1]\n",
    "    selected_indices = list(range(n_features))\n",
    "    \n",
    "    while len(selected_indices) > min_features:\n",
    "        # Find worst feature among remaining\n",
    "        remaining_scores = [scores[i] for i in selected_indices]\n",
    "        worst_idx = selected_indices[np.argmin(remaining_scores)]\n",
    "        selected_indices.remove(worst_idx)\n",
    "    \n",
    "    return np.array(selected_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:110: UserWarning: Features [12] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "C:\\Users\\ljkap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:111: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.30396835, -0.1514247 , -0.0319849 , -0.31342232, -0.46897163],\n",
       "       [ 0.70236132, -0.1514247 , -0.1206225 ,  0.91803403,  0.58108491],\n",
       "       [-0.431723  , -0.1514247 , -0.09994901, -0.60489341, -0.60227745],\n",
       "       ...,\n",
       "       [-0.48827014, -0.1514247 , -0.01673819, -0.37683259, -0.66128167],\n",
       "       [-0.77728886, -0.1514247 , -0.10201636, -0.66434054, -0.96285878],\n",
       "       [-0.85373222,  0.7894837 ,  0.10471859, -0.50257228, -0.51787096]],\n",
       "      shape=(10000, 5))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backward_selected_indices = fast_backward_elimination(X_sample, y_sample, min_features=5)\n",
    "backward_selected_data = X_sample.iloc[:, forward_selected_indices].values\n",
    "backward_selected_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-Run Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['calculatedfinishedsquarefeet', 'propertylandusetypeid', 'regionidzip',\n",
       "       'sqft_bath_interaction', 'sqft_vs_type_average'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward_selected_column_names = X_train_num_scaled2.columns[forward_selected_indices]\n",
    "forward_selected_column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression     -> MAE: 245071.393 ± 3284.876\n",
      "Bagging Regressor    -> MAE: 219407.081 ± 3149.185\n",
      "Gradient Boosting    -> MAE: 212215.773 ± 3095.143\n"
     ]
    }
   ],
   "source": [
    "forward_model_performance = evaluate_models(models_dict,X_train_num_scaled2[forward_selected_column_names], y_train )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['calculatedfinishedsquarefeet', 'propertylandusetypeid', 'regionidzip',\n",
       "       'sqft_bath_interaction', 'sqft_vs_type_average'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backward_selected_column_names = X_train_num_scaled2.columns[backward_selected_indices]\n",
    "backward_selected_column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression     -> MAE: 245071.393 ± 3284.876\n",
      "Bagging Regressor    -> MAE: 219407.081 ± 3149.185\n",
      "Gradient Boosting    -> MAE: 212215.773 ± 3095.143\n"
     ]
    }
   ],
   "source": [
    "backward_selected_columns = X_sample.columns[backward_selected_indices].tolist()\n",
    "backward_model_performance = evaluate_models(models_dict,X_train_num_scaled2[backward_selected_columns], y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Discussion [3 pts]\n",
    "\n",
    "Analyze the effect of feature selection on your models:\n",
    "\n",
    "- Did performance improve for any models after reducing the number of features?\n",
    "\n",
    "- Which features were consistently retained across models?\n",
    "\n",
    "- Were any of your newly engineered features selected as important?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perforamnce did not improved using the smaller number of features. Both forward and backward selection retained the same 5 features: calculatedfinishedsquarefeet, propertylandusetypeid, regionidzip, sqft_bath_interaction, sqft_vs_type_average. Two of the engineered features were selected as important: sqft_bath_interaction\n",
    "and sqft_vs_type_average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Fine-Tuning Your Three Models [6 pts]\n",
    "\n",
    "In this final phase of Milestone 2, you’ll select and refine your **three most promising models and their corresponding data pipelines** based on everything you've done so far, and pick a winner!\n",
    "\n",
    "1. For each of your three models:\n",
    "    - Choose your best engineered features and best selection of features as determined above. \n",
    "   - Perform hyperparameter tuning using `sweep_parameters`, `GridSearchCV`, `RandomizedSearchCV`, `Optuna`, etc. as you have practiced in previous homeworks. \n",
    "3. Decide on the best hyperparameters for each model, and for each run with repeated CV and record their final results:\n",
    "    - Report the **mean and standard deviation of CV MAE Score**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge MAE: 243545.03 ± 3092.46\n"
     ]
    }
   ],
   "source": [
    "# Add as many cells as you need\n",
    "\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
    "\n",
    "\n",
    "ridge = Ridge(random_state=42)\n",
    "param_grid_ridge = {\n",
    "    'alpha': [0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "}\n",
    "\n",
    "ridge_grid = GridSearchCV(\n",
    "    ridge, param_grid_ridge,\n",
    "    cv=cv, scoring='neg_mean_absolute_error',\n",
    "    n_jobs=-1\n",
    ")\n",
    "ridge_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "best_ridge = ridge_grid.best_estimator_\n",
    "ridge_score = -ridge_grid.best_score_\n",
    "ridge_std = ridge_grid.cv_results_['std_test_score'][ridge_grid.best_index_]\n",
    "\n",
    "print(f\"Ridge MAE: {ridge_score:.2f} ± {ridge_std:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging MAE: 188472.38 ± 2550.82\n"
     ]
    }
   ],
   "source": [
    "bagging = BaggingRegressor(random_state=42)\n",
    "param_grid_bag = {\n",
    "    'n_estimators': [10, 50, 100],\n",
    "    'max_samples': [0.5, 0.8, 1.0],\n",
    "    'max_features': [0.5, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "bag_grid = GridSearchCV(\n",
    "    bagging, param_grid_bag,\n",
    "    cv=cv, scoring='neg_mean_absolute_error',\n",
    "    n_jobs=-1\n",
    ")\n",
    "bag_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "best_bag = bag_grid.best_estimator_\n",
    "bag_score = -bag_grid.best_score_\n",
    "bag_std = bag_grid.cv_results_['std_test_score'][bag_grid.best_index_]\n",
    "\n",
    "print(f\"Bagging MAE: {bag_score:.2f} ± {bag_std:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting MAE: 190770.09 ± 2609.70\n"
     ]
    }
   ],
   "source": [
    "gbr = GradientBoostingRegressor(random_state=42)\n",
    "param_grid_gb = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'max_depth': [3, 5]\n",
    "}\n",
    "\n",
    "gb_grid = GridSearchCV(\n",
    "    gbr, param_grid_gb,\n",
    "    cv=cv, scoring='neg_mean_absolute_error',\n",
    "    n_jobs=-1\n",
    ")\n",
    "gb_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "best_gb = gb_grid.best_estimator_\n",
    "gb_score = -gb_grid.best_score_\n",
    "gb_std = gb_grid.cv_results_['std_test_score'][gb_grid.best_index_]\n",
    "\n",
    "print(f\"Gradient Boosting MAE: {gb_score:.2f} ± {gb_std:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Discussion [3 pts]\n",
    "\n",
    "Reflect on your tuning process and final results:\n",
    "\n",
    "- What was your tuning strategy for each model? Why did you choose those hyperparameters?\n",
    "- Did you find that certain types of preprocessing or feature engineering worked better with specific models?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I fined tuned ridge, bagging, and gradient boosting models using grid search with cross-validation. \n",
    "\n",
    "For ridge regression, I adjusted the regularization strength parameter to control overfitting and improve generalization. For bagging, I adjusted the number of estimators and the proportion of samples and features to balance variance reduction. In gradient boosting, I tuned the number of estimators, learning nrate, and max depth to improve performance. \n",
    "\n",
    "This resulted in bagging achieving the lowest MAE. Gradient boosting followed closely, and ridge regression performed the worst. Overall, the preprocessing steps helped create consisted features that worked well for tree-based models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5: Final Model and Design Reassessment [6 pts]\n",
    "\n",
    "In this part, you will finalize your best-performing model.  You’ll also consolidate and present the key code used to run your model on the preprocessed dataset.\n",
    "**Requirements:**\n",
    "\n",
    "- Decide one your final model among the three contestants. \n",
    "\n",
    "- Below, include all code necessary to **run your final model** on the processed dataset, reporting\n",
    "\n",
    "    - Mean and standard deviation of CV MAE Score.\n",
    "    \n",
    "    - Test score on held-out test set. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV MAE: 188472.38 ± 2550.82\n",
      "Test Set MAE: 189528.59\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RepeatedKFold, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "final_model = best_bag\n",
    "\n",
    "final_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
    "cv_scores = cross_val_score(\n",
    "    final_model,\n",
    "    X_train_scaled,\n",
    "    y_train,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    cv=cv,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "cv_mae_mean = -cv_scores.mean()\n",
    "cv_mae_std = cv_scores.std()\n",
    "\n",
    "y_pred = final_model.predict(X_test_scaled)\n",
    "test_mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(f\"CV MAE: {cv_mae_mean:.2f} ± {cv_mae_std:.2f}\")\n",
    "print(f\"Test Set MAE: {test_mae:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5: Discussion [8 pts]\n",
    "\n",
    "In this final step, your goal is to synthesize your entire modeling process and assess how your earlier decisions influenced the outcome. Please address the following:\n",
    "\n",
    "1. Model Selection:\n",
    "- Clearly state which model you selected as your final model and why.\n",
    "\n",
    "- What metrics or observations led you to this decision?\n",
    "\n",
    "- Were there trade-offs (e.g., interpretability vs. performance) that influenced your choice?\n",
    "\n",
    "2. Revisiting an Early Decision\n",
    "\n",
    "- Identify one specific preprocessing or feature engineering decision from Milestone 1 (e.g., how you handled missing values, how you scaled or encoded a variable, or whether you created interaction or polynomial terms).\n",
    "\n",
    "- Explain the rationale for that decision at the time: What were you hoping it would achieve?\n",
    "\n",
    "- Now that you've seen the full modeling pipeline and final results, reflect on whether this step helped or hindered performance. Did you keep it, modify it, or remove it?\n",
    "\n",
    "- Justify your final decision with evidence—such as validation scores, visualizations, or model diagnostics.\n",
    "\n",
    "3. Lessons Learned\n",
    "\n",
    "- What insights did you gain about your dataset or your modeling process through this end-to-end workflow?\n",
    "\n",
    "- If you had more time or data, what would you explore next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Model Selection\n",
    "\n",
    "Of the three models selected, the bagging regressor had the best performance during cross-validation. It's MAE was slightly better than gradient boosting and also performed well on the held-out test set, closely aligning with the CV performance. The consistent results confirmed its choice as the final model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Revisting an Early Decision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the decisions we decided to implement in Milestone 1 was to use create interaction terms. We decided this because interaction terms help determine the value of a particular feature based on other features. Real estate in particular is an industry where features typically do not have a lone affect. Polynomial terms typically would help us determine whether or not a particular feature has an exponential relationship with price, an unlikely scenario. \n",
    "\n",
    "This decision ended up having the affect we had hoped. Model performance after adding the interaction features improved by more than 10,000 units in both the bagging regressor model and the gradient boosting model. Additionally, the forward and backward selection models both chose to include two of our itneraction features. This is great evidence that these new features are key to creating a more accurate model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Lessons Learned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We gained several insights about our data and modeling process through this end-to-end workflow. First, we found that this particular industry has relationships that are often non-linear, complex relationships. Tree-based models and interaction features are seemingly the best approaches to model creation and feature engineering. \n",
    "\n",
    "We also found that hyperparameter tuning is extremely important to a good model. We saw hige gains in the our MAE through grid search. \n",
    "\n",
    "If we had mroe time, we would look to deploy more complex feature engineering methods, looking at possible relationships between location and price, as well as season affects on price. We would also look to create more sophisitcated ensemble methods that combine our most succesful models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
